# Start to learn RNN

## RNN basement
- [x] [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) [(译)递归神经网络不可思议的有效性](https://www.csdn.net/article/2015-08-28/2825569)
- [x] [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) [（译）理解 LSTM 网络](https://blog.csdn.net/jerr__y/article/details/58598296)
- [ ] [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

## seq2seq papers

- [ ] [Cho et al., 2014 . Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
- [ ] [Sutskever et al., 2014. Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [ ] [Bahdanau et al., 2014. Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [ ] [Jean et. al., 2014. On Using Very Large Target Vocabulary for Neural Machine Translation](https://arxiv.org/abs/1412.2007)
- [ ] [Vinyals et. al., 2015. A Neural Conversational Model[J]. Computer Science](https://arxiv.org/pdf/1506.05869v1.pdf)

## [CS224n](http://web.stanford.edu/class/cs224n/syllabus.html)

### introduction to NLP and Deep Learning 

- [x] [Linear Algebra Review](http://web.stanford.edu/class/cs224n/readings/cs229-linalg.pdf)
- [x] [Probability Review](http://web.stanford.edu/class/cs224n/readings/cs229-prob.pdf)
- [ ] [Convex Optimization Review](http://web.stanford.edu/class/cs224n/readings/cs229-cvxopt.pdf)
- [ ] [More Optimization (SGD) Review](http://cs231n.github.io/optimization-1/)

### Word Vectors

- [ ] [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
- [ ] [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
- [ ] [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)
- [ ] [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf)
- [ ] [Improving Distributional Similarity with Lessons Learned fromWord Embeddings](http://www.aclweb.org/anthology/Q15-1016)
- [ ] [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)

### Neural Networks 

- [ ] cs231n notes on [backprop](http://cs231n.github.io/optimization-2/) and [network architectures](http://cs231n.github.io/neural-networks-1/)
- [ ] [Review of differential calculus](http://web.stanford.edu/class/cs224n/readings/review-differential-calculus.pdf)
- [ ] [Natural Language Processing (almost) from Scratch](https://arxiv.org/pdf/1103.0398v1.pdf)
- [ ] [Learning Representations by Backpropagating Errors](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)

### Backpropagation and Project Advice

- [ ] [Derivatives, Backpropagation, and Vectorization](http://cs231n.stanford.edu/handouts/derivatives.pdf)
- [ ] [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)

### Introduction to TensorFlow 

- [ ] [TensorFlow Basic Usage](https://www.tensorflow.org/get_started/basic_usage)

### Dependency Parsing 

- [ ] Joakim Nivre. 2004. [Incrementality in Deterministic Dependency Parsing](https://www.aclweb.org/anthology/W/W04/W04-0308.pdf). Workshop on Incremental Parsing.
- [ ] Danqi Chen and Christopher D. Manning. 2014. [A Fast and Accurate Dependency Parser using Neural Networks](http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf). EMNLP 2014.
- [ ] Sandra Kübler, Ryan McDonald, Joakim Nivre. 2009. [Dependency Parsing](http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002). Morgan and Claypool. [Free access from Stanford campus, only!]
- [ ] Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. [Globally Normalized Transition-Based Neural Networks](https://arxiv.org/pdf/1603.06042.pdf). ACL 2016.
- [ ] Marie-Catherine de Marneffe, Timothy Dozat, Natalia Silveira, Katri Haverinen, Filip Ginter, Joakim Nivre, and Christopher D. Manning. 2014. [Universal Stanford Dependencies: A cross-linguistic typology. Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014). Revised version for UD v1](http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf).
- [ ] [Universal Dependencies website](http://universaldependencies.org/)

### Recurrent Neural Networks and Language Models

- [ ] [N-gram Language Models and Perplexity](https://web.stanford.edu/~jurafsky/slp3/4.pdf)
- [ ] [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [ ] [Recurrent Neural Networks Tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
- [ ] [Sequence Modeling: Recurrent and Recursive Neural Nets](http://www.deeplearningbook.org/contents/rnn.html)

### Vanishing Gradients, Fancy RNNs 

- [ ] [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [ ] [Vanishing Gradients Example](http://web.stanford.edu/class/cs224n/archive/WWW_1617/lectures/vanishing_grad_example.html)

### Machine Translation, Seq2Seq and Attention

- [ ] [Statistical Machine Translation slides (see lectures 2/3/4)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/syllabus.shtml)
- [ ] [Statistical Machine Translation Book](http://www.statmt.org/book/)
- [ ] [BLEU metric](https://www.aclweb.org/anthology/P02-1040.pdf)
- [ ] [Original sequence-to-sequence NMT paper (also describes beam search)](https://arxiv.org/pdf/1409.3215.pdf)
- [ ] [Earlier sequence-to-sequence speech recognition paper (includes detailed beam search alg)](https://arxiv.org/pdf/1211.3711.pdf)
- [ ] [Original sequence-to-sequence + attention paper](https://arxiv.org/pdf/1409.0473.pdf)
- [ ] [Guide to attention and other RNN augmentations](https://distill.pub/2016/augmented-rnns/)
- [ ] [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf)

### Advanced Attention 

- [ ] [A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304)
- [ ] [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)
- [ ] [BlackOut: Speeding up Recurrent Neural Network Language Models with very Large Vocabularies](https://arxiv.org/abs/1511.06909)
- [ ] [Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models](https://arxiv.org/abs/1604.00788)
- [ ] [Quasi-Recurrent Neural Networks](https://arxiv.org/abs/1611.01576)

### Networks and CNNs

- [ ] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [ ] [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)
- [ ] [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
- [ ] [Improving neural network3s by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580)
- [ ] [A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/pdf/1404.2188.pdf)

### Coreference Resolution

- [ ] [Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution](http://people.seas.harvard.edu/~srush/acl15.pdf)
- [ ] [Improving Coreference Resolution by Learning Entity-Level Distributed Representations](https://cs.stanford.edu/~kevclark/resources/clark-manning-acl16-improving.pdf)
- [ ] [End-to-end Neural Coreference Resolution](https://arxiv.org/pdf/1707.07045.pdf)
- [ ] [Coreference Demo](https://huggingface.co/coref/)

### Tree Recursive Neural Networks and Constituency Parsing 

### Advanced Architectures and Memory Networks 

### Reinforcement Learning for NLP Guest Lecture 

- [ ] [A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304)
- [ ] [DCN+: Mixed Objective and Deep Residual Coattention for Question Answering](https://arxiv.org/abs/1711.00106)
- [ ] [Deep Reinforcement Learning for Dialogue Generation](https://arxiv.org/pdf/1606.01541.pdf)

### Semi-supervised Learning for NLP

- [ ] [Semi-Supervised Sequence Learning](https://arxiv.org/abs/1511.01432)
- [ ] [Learned in Translation: Contextualized Word Vectors](https://arxiv.org/pdf/1708.00107.pdf)
- [ ] [Deep Contextualized Word Representations](https://arxiv.org/pdf/1802.05365.pdf)
- [ ] [Adversarial Training Methods for Semi-Supervised Text Classification](https://arxiv.org/pdf/1605.07725.pdf)

### Future of NLP Models, Multi-task Learning and QA Systems 
